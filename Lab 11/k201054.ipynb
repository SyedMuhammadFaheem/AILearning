{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4URFbQL6URQT298mxmBfY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import gym\n","\n","env = gym.make('FrozenLake-v1')\n","\n","alpha = 0.1\n","gamma = 0.9\n","epsilon= 0.5\n","\n","num_episodes = 100000\n","\n","state = env.observation_space.n\n","action = env.action_space.n\n","\n","Q = np.zeros((env.observation_space.n,env.action_space.n))\n","\n","\n","for i in range(num_episodes):\n","  state = env.reset()\n","  done = False\n","  total_reward = 0\n","\n","  while not done:\n","\n","    if np.random.uniform(0,1) < epsilon:\n","\n","      action = env.action_space.sample()\n","\n","    else:\n","      action = np.argmax(Q[state,:])\n","    \n","    next_state,reward,done,info = env.step(action)\n","    \n","    Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n","\n","    state = next_state\n","\n","\n","\n","\n","num_test_episodes = 1000\n","num_test_steps = 1000\n","num_successes = 0\n","\n","for i in range(num_test_episodes):\n","  state = env.reset()\n","  done = False\n","  steps = 0\n","\n","  while not done and steps < num_test_steps:\n"," \n","    action = np.argmax(Q[state, :])\n","\n","    \n","    next_state, reward, done, info = env.step(action)\n","\n","  \n","    state = next_state\n","    steps += 1\n","\n","  if state == 15:\n","    num_successes += 1\n","\n","print(\"Success rate:\", num_successes/num_test_episodes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mURRqRIxc6Gq","executionInfo":{"status":"ok","timestamp":1683367663594,"user_tz":-300,"elapsed":40863,"user":{"displayName":"k200160 Agha Maarij Amir","userId":"09258146691876642798"}},"outputId":"3d922ea1-3a5c-46f0-bfe0-37cd945392da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Success rate: 0.096\n"]}]},{"cell_type":"code","source":["import gym\n","\n","\n","env = gym.make('Taxi-v3')\n","\n","\n","state = env.observation_space.n\n","action = env.action_space.n\n","\n","\n","\n","\n","\n","\n","def choose_action(state, Q, epsilon):\n","    if np.random.uniform(0, 1) < epsilon:\n","        # Choose a random action\n","        return env.action_space.sample()\n","    else:\n","       \n","        return np.argmax(Q[state, :])\n","\n","\n","Q = np.zeros((state,action))\n","\n","\n","alpha = 0.1\n","gamma = 0.9\n","epsilon = 0.5\n","num_episodes = 100000\n","\n","\n","for i in range(num_episodes):\n","    \n","    state = env.reset()\n","    done = False\n","    while not done:\n","        \n","        action = choose_action(state, Q, epsilon)\n","        \n","        next, reward, done, info = env.step(action)\n","        \n","        Q[state, action] += alpha * (reward + gamma * np.max(Q[next, :]) - Q[state, action])\n","       \n","        state = next_state\n","\n","\n","num_episodes = 1000\n","total_reward = 0\n","for i in range(num_episodes):\n","    \n","    state = env.reset()\n","    done = False\n","    while not done:\n","        \n","        action = np.argmax(Q[state, :])\n","        \n","        next_state, reward, done, info = env.step(action)\n","        \n","        total_reward += reward\n","        \n","        state = next_state\n","\n","\n","print(\"Average reward per episode:\", total_reward / num_episodes)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ba2r0KLlj-HE","executionInfo":{"status":"ok","timestamp":1683370803346,"user_tz":-300,"elapsed":1241761,"user":{"displayName":"k200160 Agha Maarij Amir","userId":"09258146691876642798"}},"outputId":"b95b29bb-85d7-4b3f-8444-493d01ed6e8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average reward per episode: -200.0\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import gym\n","\n","env = gym.make('FrozenLake-v1')\n","\n","alpha = 0.1\n","gamma = 0.9\n","epsilon= 0.5\n","\n","num_episodes = 100000\n","\n","state = env.observation_space.n\n","action = env.action_space.n\n","\n","Q = np.zeros((env.observation_space.n,env.action_space.n))\n","\n","\n","for i in range(num_episodes):\n","  state = env.reset()\n","  done = False\n","  total_reward = 0\n","\n","  while not done:\n","\n","    if np.random.uniform(0,1) < epsilon:\n","\n","      action = env.action_space.sample()\n","\n","    else:\n","      action = np.argmax(Q[state,:])\n","    \n","    next_state,reward,done,info = env.step(action)\n","    \n","    Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n","\n","    state = next_state\n","\n","\n","\n","\n","num_test_episodes = 1000\n","num_test_steps = 1000\n","num_successes = 0\n","\n","for i in range(num_test_episodes):\n","  state = env.reset()\n","  done = False\n","  steps = 0\n","\n","  while not done and steps < num_test_steps:\n"," \n","    action = np.argmax(Q[state, :])\n","\n","    \n","    next_state, reward, done, info = env.step(action)\n","\n","  \n","    state = next_state\n","    steps += 1\n","\n","  if state == 15:\n","    num_successes += 1\n","\n","print(\"Success rate:\", num_successes/num_test_episodes)"],"metadata":{"id":"cA7cLY90tIfL"},"execution_count":null,"outputs":[]}]}